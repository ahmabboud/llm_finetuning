{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import AzureChatOpenAI,AzureOpenAI\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.output_parsers import JsonOutputParser\n",
    "from langchain_core.pydantic_v1 import BaseModel, Field\n",
    "import uuid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "# load .env file\n",
    "load_dotenv(override=True)\n",
    "os.environ[\"OPENAI_API_TYPE\"] = \"azure\"\n",
    "os.environ[\"OPENAI_API_KEY\"] = os.getenv('AZURE_OPENAI_KEY')\n",
    "os.environ[\"AZURE_OPENAI_ENDPOINT\"] = os.getenv('AZURE_OPENAI_BASE')\n",
    "os.environ[\"OPENAI_API_VERSION\"] = os.getenv('AZURE_OPENAI_API_VERSION')\n",
    "deployment_name = \"testimationgpt4\"  \n",
    "model = \"gpt-4\"\n",
    "# Embeddings\n",
    "os.environ[\"EMBEDDING_AZURE_OPNAI_KEY\"] = os.getenv('EMBEDDING_AZURE_OPNAI_KEY')\n",
    "os.environ[\"EMBEDDING_BASE\"] = os.getenv('EMBEDDING_BASE')\n",
    "os.environ[\"EMBEDDING_API_VERSION\"] = os.getenv('EMBEDDING_API_VERSION')\n",
    "os.environ[\"EMBEDDING_DEPLOYMENT\"] = os.getenv('EMBEDDING_DEPLOYMENT')\n",
    "os.environ[\"EMBEDDING_MODEL_NAME\"] = os.getenv('EMBEDDING_MODEL_NAME')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator_llm = AzureChatOpenAI(\n",
    "            deployment_name=deployment_name,\n",
    "            model=model,\n",
    "            temperature=0.7,\n",
    "            model_kwargs={'top_p': 0.05}\n",
    "        ) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generator Chain\n",
    "class GeneratorDataModel(BaseModel):\n",
    "    generated_answer: str = Field(description=\"Description of the answer.\")\n",
    "\n",
    "\n",
    "\n",
    "generator_template = \"\"\"Answer the query:{request}\\n{{format_instructions}}\\n\"\"\"\n",
    "\n",
    "generator_parser = JsonOutputParser(pydantic_object=GeneratorDataModel)\n",
    "generator_prompt = PromptTemplate(\n",
    "    template=generator_template,\n",
    "    input_variables=[\"request\"],\n",
    "    partial_variables={\"format_instructions\": generator_parser.get_format_instructions()},\n",
    ")\n",
    "generator_chain = generator_prompt | generator_llm | generator_parser\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "auto_transaction_id=str(uuid.uuid4())\n",
    "\n",
    "request=\"Generate only one User Story title for the login process of a banking application.\"\n",
    "input={\"generator_id\": auto_transaction_id, \"request\":request} \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'generated_answer': 'As a user, I want to securely log into my banking application so that I can access and manage my account.'}"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generator_chain.invoke(input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chat History with Retrival"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install bs4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install langchain-chroma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://python.langchain.com/v0.1/docs/use_cases/question_answering/chat_history/\n",
    "import bs4\n",
    "from langchain.chains import create_history_aware_retriever, create_retrieval_chain\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_community.chat_message_histories import ChatMessageHistory\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain_core.chat_history import BaseChatMessageHistory\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
    "from langchain_openai import AzureChatOpenAI, AzureOpenAIEmbeddings\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain.document_loaders import CSVLoader\n",
    "from langchain_community.document_loaders import DirectoryLoader\n",
    "\n",
    "llm =  AzureChatOpenAI(\n",
    "            deployment_name=deployment_name,\n",
    "            model=model,\n",
    "            temperature=0.7,\n",
    "            model_kwargs={'top_p': 0.05})\n",
    "\n",
    "# Embedding parameters\n",
    "embedding_deployment=\"testimationembedding\" \n",
    "embedding_model_name=\"text-embedding-ada-002\"\n",
    "# Embedding Model\n",
    "# defining azure openai embeddings\n",
    "embedding_model = AzureOpenAIEmbeddings(api_key=os.getenv('EMBEDDING_AZURE_OPNAI_KEY'),\n",
    "                                        azure_deployment=os.getenv('EMBEDDING_DEPLOYMENT'),\n",
    "                                        azure_endpoint=os.getenv('EMBEDDING_BASE'),\n",
    "                                        model=os.getenv('EMBEDDING_MODEL_NAME'))\n",
    "\n",
    "\n",
    "### Construct retriever ###\n",
    "\n",
    "## Web Loader   ##\n",
    "# loader = WebBaseLoader(\n",
    "#     web_paths=(\"https://lilianweng.github.io/posts/2023-06-23-agent/\",),\n",
    "#     bs_kwargs=dict(\n",
    "#         parse_only=bs4.SoupStrainer(\n",
    "#             class_=(\"post-content\", \"post-title\", \"post-header\")\n",
    "#         )\n",
    "#     ),\n",
    "# )\n",
    "\n",
    "\n",
    "## CSV Loader   ##\n",
    "# loader= CSVLoader(\"data/banklist.csv\", encoding=\"windows-1252\")\n",
    "\n",
    "\n",
    "## Directory Loader  ###\n",
    "# ref: https://python.langchain.com/v0.2/docs/how_to/document_loader_directory/\n",
    "loader = DirectoryLoader(\"data/\", glob=\"**/*.csv\", show_progress=True) #, use_multithreading=True)\n",
    "\n",
    "docs = loader.load()\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "splits = text_splitter.split_documents(docs)\n",
    "vectorstore = Chroma.from_documents(documents=splits, embedding=embedding_model)\n",
    "retriever = vectorstore.as_retriever()\n",
    "\n",
    "\n",
    "### Contextualize question ###\n",
    "contextualize_q_system_prompt = \"\"\"Given a chat history and the latest user question \\\n",
    "which might reference context in the chat history, formulate a standalone question \\\n",
    "which can be understood without the chat history. Do NOT answer the question, \\\n",
    "just reformulate it if needed and otherwise return it as is.\"\"\"\n",
    "contextualize_q_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", contextualize_q_system_prompt),\n",
    "        MessagesPlaceholder(\"chat_history\"),\n",
    "        (\"human\", \"{input}\"),\n",
    "    ]\n",
    ")\n",
    "history_aware_retriever = create_history_aware_retriever(\n",
    "    llm, retriever, contextualize_q_prompt\n",
    ")\n",
    "\n",
    "\n",
    "### Answer question ###\n",
    "qa_system_prompt = \"\"\"You are an assistant for question-answering tasks. \\\n",
    "Use the following pieces of retrieved context to answer the question. \\\n",
    "If you don't know the answer, just say that you don't know. \\\n",
    "Use three sentences maximum and keep the answer concise.\\\n",
    "\n",
    "{context}\"\"\"\n",
    "qa_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", qa_system_prompt),\n",
    "        MessagesPlaceholder(\"chat_history\"),\n",
    "        (\"human\", \"{input}\"),\n",
    "    ]\n",
    ")\n",
    "question_answer_chain = create_stuff_documents_chain(llm, qa_prompt)\n",
    "\n",
    "rag_chain = create_retrieval_chain(history_aware_retriever, question_answer_chain)\n",
    "\n",
    "\n",
    "### Statefully manage chat history ###\n",
    "store = {}\n",
    "\n",
    "\n",
    "def get_session_history(session_id: str) -> BaseChatMessageHistory:\n",
    "    if session_id not in store:\n",
    "        store[session_id] = ChatMessageHistory()\n",
    "    return store[session_id]\n",
    "\n",
    "\n",
    "conversational_rag_chain = RunnableWithMessageHistory(\n",
    "    rag_chain,\n",
    "    get_session_history,\n",
    "    input_messages_key=\"input\",\n",
    "    history_messages_key=\"chat_history\",\n",
    "    output_messages_key=\"answer\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parent run 72dd7368-117e-443b-a5d8-e320b81551bb not found for run 63a01cff-a019-4401-8baf-3d6bb73a1827. Treating as a root run.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Task Decomposition is a process where a complex task is broken down into smaller, more manageable tasks. This technique is used in models like Chain of Thought (CoT) and Tree of Thoughts to enhance performance on complex tasks by instructing the model to \"think step by step\". The process can involve Language Learning Models (LLM) with simple prompting, task-specific instructions, or human inputs.'"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversational_rag_chain.invoke(\n",
    "    {\"input\": \"What is Task Decomposition?\"},\n",
    "    config={\n",
    "        \"configurable\": {\"session_id\": \"abc123\"}\n",
    "    },  # constructs a key \"abc123\" in `store`.\n",
    ")[\"answer\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parent run 7c637a6d-f3b5-4033-bee5-97bd73ed3756 not found for run 76577efa-522f-4901-9bf2-10f1dd00fa34. Treating as a root run.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Common ways of task decomposition include: (1) using Language Learning Models (LLM) with simple prompting such as \"Steps for XYZ.\\\\n1.\", \"What are the subgoals for achieving XYZ?\", (2) using task-specific instructions, for example, \"Write a story outline.\" for writing a novel, and (3) incorporating human inputs to guide the decomposition process.'"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversational_rag_chain.invoke(\n",
    "    {\"input\": \"What are common ways of doing it?\"},\n",
    "    config={\"configurable\": {\"session_id\": \"abc123\"}},\n",
    ")[\"answer\"]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
