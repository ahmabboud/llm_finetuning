{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bba78a6d-df84-48a6-85ba-cdc1c8fa6e55",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install trl==0.6.0 transformers==4.32.0 accelerate==0.12.0 peft==0.5.0 -Uqqq\n",
    "!pip install datasets==2.13.1 bitsandbytes==0.41.1 einops==0.7.0 wandb==0.15.8 -Uqqq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7c185b49-82ce-4fa9-b1c8-4042a7a9eafc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    _|    _|  _|    _|    _|_|_|    _|_|_|  _|_|_|  _|      _|    _|_|_|      _|_|_|_|    _|_|      _|_|_|  _|_|_|_|\n",
      "    _|    _|  _|    _|  _|        _|          _|    _|_|    _|  _|            _|        _|    _|  _|        _|\n",
      "    _|_|_|_|  _|    _|  _|  _|_|  _|  _|_|    _|    _|  _|  _|  _|  _|_|      _|_|_|    _|_|_|_|  _|        _|_|_|\n",
      "    _|    _|  _|    _|  _|    _|  _|    _|    _|    _|    _|_|  _|    _|      _|        _|    _|  _|        _|\n",
      "    _|    _|    _|_|      _|_|_|    _|_|_|  _|_|_|  _|      _|    _|_|_|      _|        _|    _|    _|_|_|  _|_|_|_|\n",
      "\n",
      "    A token is already saved on your machine. Run `huggingface-cli whoami` to get more information or `huggingface-cli logout` if you want to log out.\n",
      "    Setting a new token will erase the existing one.\n",
      "    To login, `huggingface_hub` requires a token generated from https://huggingface.co/settings/tokens .\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter your token (input will not be visible):  ········\n",
      "Add token as git credential? (Y/n)  n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token is valid (permission: write).\n",
      "Your token has been saved to /h/ws_vbhagat/.cache/huggingface/token\n",
      "Login successful\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset, DatasetDict\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    BitsAndBytesConfig,\n",
    "    HfArgumentParser,\n",
    "    AutoTokenizer,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    GenerationConfig\n",
    ")\n",
    "from tqdm import tqdm\n",
    "from trl import SFTTrainer\n",
    "import torch\n",
    "import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from huggingface_hub import interpreter_login\n",
    "\n",
    "interpreter_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dc7ef33d-4db4-42e6-b7f6-5a7ba72aaacc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# disable Weights and Biases\n",
    "os.environ['WANDB_DISABLED']=\"true\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6b9c8991-a068-46f3-9915-70ec8dc676b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "huggingface_dataset_name = \"Amod/mental_health_counseling_conversations\"\n",
    "dataset = load_dataset(huggingface_dataset_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8921b850-79ee-47de-9a53-be73ed89b33a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3512\n"
     ]
    }
   ],
   "source": [
    "total_length = len(dataset['train'])\n",
    "print(total_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "60947f62-9fe8-4ec8-8049-34d55bd70b79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training examples: 2812\n",
      "Number of validation examples: 348\n",
      "Number of test examples: 352\n"
     ]
    }
   ],
   "source": [
    "\n",
    "split_dataset = dataset['train'].train_test_split(test_size=0.1)\n",
    "\n",
    "# Further split the training set into train and validation sets (e.g., 75% train, 25% validation)\n",
    "train_valid_split = split_dataset['train'].train_test_split(test_size=0.11)\n",
    "\n",
    "# Combine the splits into a DatasetDict\n",
    "dataset = DatasetDict({\n",
    "    'train': train_valid_split['train'],\n",
    "    'validation': train_valid_split['test'],\n",
    "    'test': split_dataset['test']\n",
    "})\n",
    "\n",
    "# Print the number of examples in each split\n",
    "print(f\"Number of training examples: {len(dataset['train'])}\")\n",
    "print(f\"Number of validation examples: {len(dataset['validation'])}\")\n",
    "print(f\"Number of test examples: {len(dataset['test'])}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "686d8a33-c31d-43f2-a0a3-17af5dfe948c",
   "metadata": {},
   "outputs": [],
   "source": [
    "compute_dtype = getattr(torch, \"float16\")\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "        load_in_4bit=True,\n",
    "        bnb_4bit_quant_type='nf4',\n",
    "        bnb_4bit_compute_dtype=compute_dtype,\n",
    "        bnb_4bit_use_double_quant=False,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f133afde-c094-42ac-8969-45408684e4f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/fs01/projects/fta_bootcamp/envs/finetune_demo/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py:468: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a59cebce84c04005a0375b0ee9c5fb90",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "os.chdir(\"../../../../../../\")\n",
    "os.getcwd()\n",
    "\n",
    "model_name=\"/fs01/model-weights/Meta-Llama-3-8B-Instruct\"\n",
    "device_map = {\"\": 0}\n",
    "original_model = AutoModelForCausalLM.from_pretrained(model_name, \n",
    "                                                      device_map=device_map,\n",
    "                                                      quantization_config=bnb_config,\n",
    "                                                      trust_remote_code=True,\n",
    "                                                      use_auth_token=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "025b548f-24ad-4911-a91c-60fa72c7dd21",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_name,trust_remote_code=True,padding_side=\"left\",add_eos_token=True,add_bos_token=True,use_fast=False)\n",
    "tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6f96d9d5-1bd5-409f-8e4d-32bf2bd531df",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'gen' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "File \u001b[0;32m<timed exec>:11\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'gen' is not defined"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from transformers import set_seed\n",
    "seed = 42\n",
    "set_seed(seed)\n",
    "\n",
    "index = 10\n",
    "\n",
    "prompt = dataset['test'][index]['Context']\n",
    "summary = dataset['test'][index]['Response']\n",
    "\n",
    "formatted_prompt = f\"Instruct: Answer the following mental health related question.\\n{prompt}\\nOutput:\\n\"\n",
    "res = gen(original_model,formatted_prompt,100,)\n",
    "#print(res[0])\n",
    "output = res[0].split('Output:\\n')[1]\n",
    "\n",
    "dash_line = '-'.join('' for x in range(100))\n",
    "print(dash_line)\n",
    "print(f'INPUT PROMPT:\\n{formatted_prompt}')\n",
    "print(dash_line)\n",
    "print(f'BASELINE HUMAN ANSWER:\\n{summary}\\n')\n",
    "print(dash_line)\n",
    "print(f'MODEL GENERATION - ZERO SHOT:\\n{output}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a7e59b58-a053-4075-b66f-3fd5b6b52ef5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_prompt_formats(sample):\n",
    "    \"\"\"\n",
    "    Format various fields of the sample ('instruction','output')\n",
    "    Then concatenate them using two newline characters \n",
    "    :param sample: Sample dictionnary\n",
    "    \"\"\"\n",
    "    INTRO_BLURB = \"Below is an instruction that describes a task. Write a response that appropriately completes the request.\"\n",
    "    INSTRUCTION_KEY = \"### Instruct: Answer the following question.\"\n",
    "    RESPONSE_KEY = \"### Output:\"\n",
    "    END_KEY = \"### End\"\n",
    "    \n",
    "    blurb = f\"\\n{INTRO_BLURB}\"\n",
    "    instruction = f\"{INSTRUCTION_KEY}\"\n",
    "    input_context = f\"{sample['Context']}\" if sample[\"Context\"] else None\n",
    "    response = f\"{RESPONSE_KEY}\\n{sample['Response']}\"\n",
    "    end = f\"{END_KEY}\"\n",
    "    \n",
    "    parts = [part for part in [blurb, instruction, input_context, response, end] if part]\n",
    "\n",
    "    formatted_prompt = \"\\n\\n\".join(parts)\n",
    "    sample[\"text\"] = formatted_prompt\n",
    "\n",
    "    return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c3156777-e8fc-4355-966a-429432817189",
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "\n",
    "# SOURCE https://github.com/databrickslabs/dolly/blob/master/training/trainer.py\n",
    "def get_max_length(model):\n",
    "    conf = model.config\n",
    "    max_length = None\n",
    "    for length_setting in [\"n_positions\", \"max_position_embeddings\", \"seq_length\"]:\n",
    "        max_length = getattr(model.config, length_setting, None)\n",
    "        if max_length:\n",
    "            print(f\"Found max lenth: {max_length}\")\n",
    "            break\n",
    "    if not max_length:\n",
    "        max_length = 1024\n",
    "        print(f\"Using default max length: {max_length}\")\n",
    "    return max_length\n",
    "\n",
    "\n",
    "def preprocess_batch(batch, tokenizer, max_length):\n",
    "    \"\"\"\n",
    "    Tokenizing a batch\n",
    "    \"\"\"\n",
    "    return tokenizer(\n",
    "        batch[\"text\"],\n",
    "        max_length=max_length,\n",
    "        truncation=True,\n",
    "    )\n",
    "\n",
    "# SOURCE https://github.com/databrickslabs/dolly/blob/master/training/trainer.py\n",
    "def preprocess_dataset(tokenizer: AutoTokenizer, max_length: int,seed, dataset):\n",
    "    \"\"\"Format & tokenize it so it is ready for training\n",
    "    :param tokenizer (AutoTokenizer): Model Tokenizer\n",
    "    :param max_length (int): Maximum number of tokens to emit from tokenizer\n",
    "    \"\"\"\n",
    "    \n",
    "    # Add prompt to each sample\n",
    "    print(\"Preprocessing dataset...\")\n",
    "    dataset = dataset.map(create_prompt_formats)#, batched=True)\n",
    "    \n",
    "    # Apply preprocessing to each batch of the dataset & and remove 'instruction', 'context', 'response', 'category' fields\n",
    "    _preprocessing_function = partial(preprocess_batch, max_length=max_length, tokenizer=tokenizer)\n",
    "    dataset = dataset.map(\n",
    "        _preprocessing_function,\n",
    "        batched=True,\n",
    "        remove_columns=['Context', 'Response'],\n",
    "    )\n",
    "\n",
    "    # Filter out samples that have input_ids exceeding max_length\n",
    "    dataset = dataset.filter(lambda sample: len(sample[\"input_ids\"]) < max_length)\n",
    "    \n",
    "    # Shuffle dataset\n",
    "    dataset = dataset.shuffle(seed=seed)\n",
    "\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8b8fb1a2-7e1f-4725-873c-bb586c960ca9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found max lenth: 8192\n",
      "8192\n",
      "Preprocessing dataset...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "961faecf516442749b510de1e2ad04b6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2812 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ddcf7bd3aa4c44a1bb09fa7cc9285fe5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2812 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8ef64d8f405c405c95244b1759ad3a4e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/2812 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessing dataset...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ab89092eb90643019f9d12279a33c74a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/348 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0bc918e11efe423581c6a7dbb276173c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/348 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c1286c7fc9a14abdb51eeebda1c15bab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/348 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "## Pre-process dataset\n",
    "max_length = get_max_length(original_model)\n",
    "print(max_length)\n",
    "\n",
    "train_dataset = preprocess_dataset(tokenizer, max_length,seed, dataset['train'])\n",
    "eval_dataset = preprocess_dataset(tokenizer, max_length,seed, dataset['validation'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1d76a60e-8dca-45de-9a87-1677376ced71",
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
    "\n",
    "config = LoraConfig(\n",
    "    r=32, #Rank\n",
    "    lora_alpha=32,\n",
    "    target_modules=[\n",
    "        'q_proj',\n",
    "        'k_proj',\n",
    "        'v_proj',\n",
    "        'dense'\n",
    "    ],\n",
    "    bias=\"none\",\n",
    "    lora_dropout=0.05,  # Conventional\n",
    "    task_type=\"CAUSAL_LM\",\n",
    ")\n",
    "\n",
    "# 1 - Enabling gradient checkpointing to reduce memory usage during fine-tuning\n",
    "original_model.gradient_checkpointing_enable()\n",
    "\n",
    "peft_model = get_peft_model(original_model, config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b96e05f6-f9e1-47b1-8ba5-334f89f59307",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2 - Using the prepare_model_for_kbit_training method from PEFT\n",
    "# Preparing the Model for QLoRA\n",
    "original_model = prepare_model_for_kbit_training(original_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "70bfc02d-8d4a-4e9b-9cde-ddc738f73757",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'print_number_of_trainable_model_parameters' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[19], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mprint_number_of_trainable_model_parameters\u001b[49m(peft_model))\n",
      "\u001b[0;31mNameError\u001b[0m: name 'print_number_of_trainable_model_parameters' is not defined"
     ]
    }
   ],
   "source": [
    "print(print_number_of_trainable_model_parameters(peft_model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "148dc899-0d2c-40f1-9078-a4bfa724590b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n",
      "max_steps is given, it will override any value given in num_train_epochs\n"
     ]
    }
   ],
   "source": [
    "output_dir = f'/h/ws_vbhagat/llm_finetuning/Deloitte/finetuned/Vijay_FineTuning/Vijay_FineTuning_Checkpoints-{str(int(time.time()))}'\n",
    "import transformers\n",
    "\n",
    "peft_training_args = TrainingArguments(\n",
    "    output_dir = output_dir,\n",
    "    warmup_steps=1,\n",
    "    per_device_train_batch_size=1,\n",
    "    gradient_accumulation_steps=4,\n",
    "    max_steps=1000,\n",
    "    learning_rate=2e-4,\n",
    "    optim=\"paged_adamw_8bit\",\n",
    "    logging_steps=25,\n",
    "    logging_dir=\"./logs\",\n",
    "    save_strategy=\"steps\",\n",
    "    save_steps=25,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    eval_steps=25,\n",
    "    do_eval=True,\n",
    "    gradient_checkpointing=True,\n",
    "    report_to=\"none\",\n",
    "    overwrite_output_dir = 'True',\n",
    "    group_by_length=True,\n",
    ")\n",
    "\n",
    "peft_model.config.use_cache = False\n",
    "\n",
    "peft_trainer = transformers.Trainer(\n",
    "    model=peft_model,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    "    args=peft_training_args,\n",
    "    data_collator=transformers.DataCollatorForLanguageModeling(tokenizer, mlm=False),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "713a03b2-4ffa-42ff-af47-9fe42399bb0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/fs01/projects/fta_bootcamp/envs/finetune_demo/lib/python3.10/site-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='176' max='1000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 176/1000 34:49 < 2:44:55, 0.08 it/s, Epoch 0.25/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>2.381800</td>\n",
       "      <td>2.278322</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>2.092800</td>\n",
       "      <td>2.270487</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>75</td>\n",
       "      <td>2.140500</td>\n",
       "      <td>2.212942</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>2.084300</td>\n",
       "      <td>2.228540</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>125</td>\n",
       "      <td>2.172200</td>\n",
       "      <td>2.204546</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>1.965600</td>\n",
       "      <td>2.221710</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>\n",
       "    <div>\n",
       "      \n",
       "      <progress value='24' max='44' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [24/44 01:47 < 01:33, 0.21 it/s]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/fs01/projects/fta_bootcamp/envs/finetune_demo/lib/python3.10/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in /fs01/model-weights/Meta-Llama-3-8B-Instruct - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n",
      "/fs01/projects/fta_bootcamp/envs/finetune_demo/lib/python3.10/site-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "/fs01/projects/fta_bootcamp/envs/finetune_demo/lib/python3.10/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in /fs01/model-weights/Meta-Llama-3-8B-Instruct - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n",
      "/fs01/projects/fta_bootcamp/envs/finetune_demo/lib/python3.10/site-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "/fs01/projects/fta_bootcamp/envs/finetune_demo/lib/python3.10/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in /fs01/model-weights/Meta-Llama-3-8B-Instruct - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n",
      "/fs01/projects/fta_bootcamp/envs/finetune_demo/lib/python3.10/site-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "/fs01/projects/fta_bootcamp/envs/finetune_demo/lib/python3.10/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in /fs01/model-weights/Meta-Llama-3-8B-Instruct - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n",
      "/fs01/projects/fta_bootcamp/envs/finetune_demo/lib/python3.10/site-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "/fs01/projects/fta_bootcamp/envs/finetune_demo/lib/python3.10/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in /fs01/model-weights/Meta-Llama-3-8B-Instruct - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n",
      "/fs01/projects/fta_bootcamp/envs/finetune_demo/lib/python3.10/site-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "/fs01/projects/fta_bootcamp/envs/finetune_demo/lib/python3.10/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in /fs01/model-weights/Meta-Llama-3-8B-Instruct - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n",
      "/fs01/projects/fta_bootcamp/envs/finetune_demo/lib/python3.10/site-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "peft_trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "efd96eb9-b428-4e10-a243-0bf6ca8cdfd6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/fs01/projects/fta_bootcamp/envs/finetune_demo/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py:468: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7aad4b64871343cdb143530b42b5d7ae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "base_model_id = \"/fs01/model-weights/Meta-Llama-3-8B-Instruct\"\n",
    "base_model = AutoModelForCausalLM.from_pretrained(base_model_id, \n",
    "                                                      device_map='auto',\n",
    "                                                      quantization_config=bnb_config,\n",
    "                                                      trust_remote_code=True,\n",
    "                                                      use_auth_token=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "059ecd9b-1b62-418d-839f-e397505a71d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import PeftModel\n",
    "\n",
    "ft_model = PeftModel.from_pretrained(base_model, \"/h/ws_vbhagat/llm_finetuning/Deloitte/finetuned/Vijay_FineTuning/Vijay_FineTuning_Checkpoints-1721306631/checkpoint-125\",torch_dtype=torch.float16,is_trainable=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "239dd925-d671-430c-b828-732fc51cabef",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'str' object has no attribute 'args'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[26], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtransformers\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave_model\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m/h/ws_vbhagat/llm_finetuning/Deloitte/finetuned/Vijay_FineTuning/\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/fs01/projects/fta_bootcamp/envs/finetune_demo/lib/python3.10/site-packages/transformers/trainer.py:3215\u001b[0m, in \u001b[0;36mTrainer.save_model\u001b[0;34m(self, output_dir, _internal_call)\u001b[0m\n\u001b[1;32m   3208\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   3209\u001b[0m \u001b[38;5;124;03mWill save the model, so you can reload it using `from_pretrained()`.\u001b[39;00m\n\u001b[1;32m   3210\u001b[0m \n\u001b[1;32m   3211\u001b[0m \u001b[38;5;124;03mWill only save from the main process.\u001b[39;00m\n\u001b[1;32m   3212\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   3214\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m output_dir \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 3215\u001b[0m     output_dir \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43margs\u001b[49m\u001b[38;5;241m.\u001b[39moutput_dir\n\u001b[1;32m   3217\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_torch_xla_available():\n\u001b[1;32m   3218\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_save_tpu(output_dir)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'str' object has no attribute 'args'"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "036d724f-3c73-44b2-a203-8becc7289b69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to /h/ws_vbhagat/llm_finetuning/Deloitte/finetuned/Vijay_FineTuning/Vijay_FineTuning_SavedModels/llama3-8b-finetuned-mentalwellness-v1.pth\n"
     ]
    }
   ],
   "source": [
    "model_save_path = \"/h/ws_vbhagat/llm_finetuning/Deloitte/finetuned/Vijay_FineTuning/Vijay_FineTuning_SavedModels/llama3-8b-finetuned-mentalwellness-v1.pth\"\n",
    "torch.save(ft_model.state_dict(), model_save_path)\n",
    "\n",
    "print(f\"Model saved to {model_save_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e83e607-3d52-4832-8bf4-40422e8c6826",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "bf1aec66-958f-4f76-b18d-1d5074cbf0c3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "original_model = AutoModelForCausalLM.from_pretrained(base_model_id, \n",
    "                                                      device_map='auto',\n",
    "                                                      quantization_config=bnb_config,\n",
    "                                                      trust_remote_code=True,\n",
    "                                                      use_auth_token=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "05179cb4-3008-4d8b-a69f-1769f3b3cae9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is a simulated response based on the prompt: Instruct: Answer the following question.\n",
      "I've gone to a couple therapy sessions so far and still everytime I walk in I get nervous and shaky.  Is this normal? Should I still be feeling like this?\n",
      "\n",
      "This is a simulated response based on the prompt: Instruct: Answer the following question.\n",
      "I want a secure relationship with someone that wants to be with me and who will actually put effort into it.\n",
      " I seem to gravitate toward unavailable men and those that want intimacy and no relationship. I let men dictate and control me because they accuse me of being controlling. I let men emotionally abuse me and I am at their beck and call.\n",
      " I am not comfortable being alone or doing anything by myself. I feel I need the security of someone being around just to survive. I know what I'm doing wrong and I do it anyway just hoping things will change. How do I stop this behavior and thought process?\n",
      "\n",
      "This is a simulated response based on the prompt: Instruct: Answer the following question.\n",
      "I just wanted to get to know one so I can hear about their college experience and the courses they took. I also wanted to know if they enjoy their job and how long they were in school.\n",
      "\n",
      "This is a simulated response based on the prompt: Instruct: Answer the following question.\n",
      "I feel like I'm trying to convince myself that I'm okay when I'm not. I'm always blocking out the bad things and forgetting. I also feel like nobody cares for me and they never will. I feel truly alone.\n",
      "\n",
      "This is a simulated response based on the prompt: Instruct: Answer the following question.\n",
      "I was with my friends at the park, and we were talking and having fun. After a while, I met a girl there. We talked and flirted for a while, then we gave our numbers to each other and left. We talked for a few months, then we met up again. She kissed me, so I assumed we were dating. Then I found out she has a boyfriend.\n",
      "\n",
      "This is a simulated response based on the prompt: Instruct: Answer the following question.\n",
      "I have a child with my baby mother. She works I take care of our young son. She says she is not cheating I have not found anything but she always putting me down, telling me to get out and telling me she doesn't love me, but then the next day after our fight she says she does. I'm having a hard time because before our child she said she was raped by a family member but she never went to the hospital or the cops. Now me and my family don't talk. She's always telling me I'm annoying and just belittles me. Nothing I do is right. She says I work you watch the baby. On her days off she never cooks or cleans. I have no friends or family and a couple months ago she was confiding to some guy, but says he's not any thing to her. What do I do? I don't want to leave.\n",
      "\n",
      "This is a simulated response based on the prompt: Instruct: Answer the following question.\n",
      "I started having anxiety three months ago. I'm new to having anxiety, and it's making me depressed.\n",
      "\n",
      "This is a simulated response based on the prompt: Instruct: Answer the following question.\n",
      "My boyfriend is in recovery from drug addiction. We recently got into a fight and he has become very distant. I don't know what to do to fix the relationship.\n",
      "\n",
      "This is a simulated response based on the prompt: Instruct: Answer the following question.\n",
      "I am divorced and happily remarried. Our blended family of children are in their 20s. My youngest, age 20, continues to call me crazy and favors her dad. He was abusive to me, and I left with injuries. He never wanted kids, and I did. He paid child support only through forced wage earners. Now my daughters favor him after all the sacrifices I made for them the past 15 years as a single mom. I don’t deny them a relationship with their dad, but to be called crazy and then watch them hang out with him hurts me to the very core. I’m not sure how to handle this.\n",
      "\n",
      "This is a simulated response based on the prompt: Instruct: Answer the following question.\n",
      "We're not together, but I’m still doing things for him and we are intimate. He’s not there for his son.\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>human_baseline_responses</th>\n",
       "      <th>original_model_responses</th>\n",
       "      <th>peft_model_responses</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I would be more concerned with how is this bei...</td>\n",
       "      <td>This is a simulated response based on the prom...</td>\n",
       "      <td>This is a simulated response based on the prom...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Good for you on your keen awareness of your di...</td>\n",
       "      <td>This is a simulated response based on the prom...</td>\n",
       "      <td>This is a simulated response based on the prom...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Mental Health is an exciting and rewarding fie...</td>\n",
       "      <td>This is a simulated response based on the prom...</td>\n",
       "      <td>This is a simulated response based on the prom...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>As social creatures, we humans all long for de...</td>\n",
       "      <td>This is a simulated response based on the prom...</td>\n",
       "      <td>This is a simulated response based on the prom...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>I'm sorry for your disappointment with this gi...</td>\n",
       "      <td>This is a simulated response based on the prom...</td>\n",
       "      <td>This is a simulated response based on the prom...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>It sounds like you are in a tough situation. Y...</td>\n",
       "      <td>This is a simulated response based on the prom...</td>\n",
       "      <td>This is a simulated response based on the prom...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>One of the first steps is to manage anxiety an...</td>\n",
       "      <td>This is a simulated response based on the prom...</td>\n",
       "      <td>This is a simulated response based on the prom...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>I would first suggest you sitting down with hi...</td>\n",
       "      <td>This is a simulated response based on the prom...</td>\n",
       "      <td>This is a simulated response based on the prom...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Hi Arkansas, Your situation sounds like a case...</td>\n",
       "      <td>This is a simulated response based on the prom...</td>\n",
       "      <td>This is a simulated response based on the prom...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>You didn't ask a direct question because I fee...</td>\n",
       "      <td>This is a simulated response based on the prom...</td>\n",
       "      <td>This is a simulated response based on the prom...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            human_baseline_responses  \\\n",
       "0  I would be more concerned with how is this bei...   \n",
       "1  Good for you on your keen awareness of your di...   \n",
       "2  Mental Health is an exciting and rewarding fie...   \n",
       "3  As social creatures, we humans all long for de...   \n",
       "4  I'm sorry for your disappointment with this gi...   \n",
       "5  It sounds like you are in a tough situation. Y...   \n",
       "6  One of the first steps is to manage anxiety an...   \n",
       "7  I would first suggest you sitting down with hi...   \n",
       "8  Hi Arkansas, Your situation sounds like a case...   \n",
       "9  You didn't ask a direct question because I fee...   \n",
       "\n",
       "                            original_model_responses  \\\n",
       "0  This is a simulated response based on the prom...   \n",
       "1  This is a simulated response based on the prom...   \n",
       "2  This is a simulated response based on the prom...   \n",
       "3  This is a simulated response based on the prom...   \n",
       "4  This is a simulated response based on the prom...   \n",
       "5  This is a simulated response based on the prom...   \n",
       "6  This is a simulated response based on the prom...   \n",
       "7  This is a simulated response based on the prom...   \n",
       "8  This is a simulated response based on the prom...   \n",
       "9  This is a simulated response based on the prom...   \n",
       "\n",
       "                                peft_model_responses  \n",
       "0  This is a simulated response based on the prom...  \n",
       "1  This is a simulated response based on the prom...  \n",
       "2  This is a simulated response based on the prom...  \n",
       "3  This is a simulated response based on the prom...  \n",
       "4  This is a simulated response based on the prom...  \n",
       "5  This is a simulated response based on the prom...  \n",
       "6  This is a simulated response based on the prom...  \n",
       "7  This is a simulated response based on the prom...  \n",
       "8  This is a simulated response based on the prom...  \n",
       "9  This is a simulated response based on the prom...  "
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "contexts = dataset['test'][0:10]['Context']\n",
    "human_baseline_responses = dataset['test'][0:10]['Response']\n",
    "\n",
    "\n",
    "original_model_responses = []\n",
    "instruct_model_responses = []\n",
    "peft_model_responses = []\n",
    "\n",
    "for idx, context in enumerate(contexts):\n",
    "    human_baseline_text_output = human_baseline_responses[idx]\n",
    "    prompt = f\"Instruct: Answer the following question.\\n{context}\\nOutput:\\n\"\n",
    "    \n",
    "    original_model_res = gen(original_model,prompt,100,)\n",
    "    original_model_text_output = original_model_res[0].split('Output:\\n')[1]\n",
    "    \n",
    "    peft_model_res = gen(ft_model,prompt,100,)\n",
    "    peft_model_output = peft_model_res[0].split('Output:\\n')[1]\n",
    "    print(peft_model_output)\n",
    "    peft_model_text_output, success, result = peft_model_output.partition('###')\n",
    "\n",
    "    original_model_responses.append(original_model_text_output)\n",
    "    peft_model_responses.append(peft_model_text_output)\n",
    "\n",
    "zipped_summaries = list(zip(human_baseline_responses, original_model_responses, peft_model_responses))\n",
    " \n",
    "df = pd.DataFrame(zipped_summaries, columns = ['human_baseline_responses', 'original_model_responses', 'peft_model_responses'])\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c7e7d5e-f6b5-4059-91a5-ec6b0a2f1a93",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ced90ed-4cce-4c96-af26-6ae3e2592d60",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "eec52b62-9ab3-44db-85d6-a9fdad4aca4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mERROR: Could not find a version that satisfies the requirement gguf_library (from versions: none)\u001b[0m\u001b[31m\n",
      "\u001b[0m\u001b[31mERROR: No matching distribution found for gguf_library\u001b[0m\u001b[31m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.1.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install gguf_library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f61fb069-ce07-4b96-8e40-d3b96490b530",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "\n",
    "model = torch.load('/h/ws_vbhagat/llm_finetuning/Deloitte/finetuned/Vijay_FineTuning/Vijay_FineTuning_SavedModels/llama3-8b-finetuned-mentalwellness-v1.pth')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c37ea8f6-bca2-4297-9416-062840e45c1b",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'collections.OrderedDict' object has no attribute 'save_pretrained'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave_pretrained\u001b[49m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/h/ws_vbhagat/llm_finetuning/Deloitte/finetuned/Vijay_FineTuning/Vijay_FineTuning_SavedModels\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'collections.OrderedDict' object has no attribute 'save_pretrained'"
     ]
    }
   ],
   "source": [
    "model.save_pretrained(\"/h/ws_vbhagat/llm_finetuning/Deloitte/finetuned/Vijay_FineTuning/Vijay_FineTuning_SavedModels\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
