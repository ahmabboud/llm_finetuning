{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ad3cde1c-71ec-41a7-9cdf-d5b54b06852d",
   "metadata": {},
   "source": [
    "# Evaluate Fine-Tuned Llama Using DeepEval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "04a8e9ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!srun --gres=gpu:1 --mem=48G --account=fta_boot --qos=fta_boot --partition=a40 --pty bash\n",
    "# !scancel --user=ws_aabboud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "e0e08470",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: python-dotenv in /fs01/home/ws_aabboud/venv312/lib/python3.12/site-packages (1.0.1)\n"
     ]
    }
   ],
   "source": [
    "# !pip install evaluate\n",
    "# !pip install pandas\n",
    "# !pip install numpy\n",
    "# !pip install absl-py\n",
    "# !pip install rouge-score\n",
    "# !pip install python-dotenv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d662e3e-5c39-4494-b680-4d0a2b3889a2",
   "metadata": {},
   "source": [
    "## 1. Fine-Tuning Llama 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "5e2b8b58-2a5c-4e39-9d21-f0b5c9af729f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    BitsAndBytesConfig,\n",
    "    HfArgumentParser,\n",
    "    TrainingArguments,\n",
    "    pipeline,\n",
    "    logging,\n",
    ")\n",
    "from peft import (\n",
    "    LoraConfig,\n",
    "    PeftModel,\n",
    "    prepare_model_for_kbit_training,\n",
    "\n",
    "    get_peft_model,\n",
    ")\n",
    "import ast\n",
    "import re\n",
    "import time\n",
    "import os, torch\n",
    "from datasets import load_dataset, DatasetDict\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from trl import SFTTrainer, setup_chat_format\n",
    "from dotenv import load_dotenv \n",
    "\n",
    "from deepeval.integrations.hugging_face import DeepEvalHuggingFaceCallback\n",
    "from deepeval.metrics import AnswerRelevancyMetric, GEval\n",
    "from deepeval import evaluate\n",
    "from deepeval.dataset import EvaluationDataset, Golden\n",
    "from deepeval.test_case import LLMTestCase\n",
    "from deepeval.test_case import LLMTestCaseParams\n",
    "from deepeval.models.base_model import DeepEvalBaseLLM\n",
    "\n",
    "# from deepeval_gemini import GoogleVertexAI\n",
    "\n",
    "import torch\n",
    "import time\n",
    "import evaluate \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from transformers.generation import GenerationConfig\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "2ab4af62-fa93-4208-9d71-2068a772a285",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "home=os.path.expanduser('~')\n",
    "os.chdir(f'{home}/finetuning-and-alignment/Deloitte/finetuned/')\n",
    "# os.getcwd()\n",
    "load_dotenv(override=True)\n",
    "## OpenAI\n",
    "os.environ[\"OPENAI_API_KEY\"] =os.getenv('OPENAI_API_KEY')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "692eab3d-42da-42d9-b0aa-4cb813624f8a",
   "metadata": {},
   "source": [
    "### Login to HuggingFace Hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4be35a82-152d-4802-b34f-aa6c87aef483",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    _|    _|  _|    _|    _|_|_|    _|_|_|  _|_|_|  _|      _|    _|_|_|      _|_|_|_|    _|_|      _|_|_|  _|_|_|_|\n",
      "    _|    _|  _|    _|  _|        _|          _|    _|_|    _|  _|            _|        _|    _|  _|        _|\n",
      "    _|_|_|_|  _|    _|  _|  _|_|  _|  _|_|    _|    _|  _|  _|  _|  _|_|      _|_|_|    _|_|_|_|  _|        _|_|_|\n",
      "    _|    _|  _|    _|  _|    _|  _|    _|    _|    _|    _|_|  _|    _|      _|        _|    _|  _|        _|\n",
      "    _|    _|    _|_|      _|_|_|    _|_|_|  _|_|_|  _|      _|    _|_|_|      _|        _|    _|    _|_|_|  _|_|_|_|\n",
      "\n",
      "    A token is already saved on your machine. Run `huggingface-cli whoami` to get more information or `huggingface-cli logout` if you want to log out.\n",
      "    Setting a new token will erase the existing one.\n",
      "    To login, `huggingface_hub` requires a token generated from https://huggingface.co/settings/tokens .\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter your token (input will not be visible):  ········\n",
      "Add token as git credential? (Y/n)  n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token is valid (permission: fineGrained).\n",
      "Your token has been saved to /h/ws_jinxiao/.cache/huggingface/token\n",
      "Login successful\n"
     ]
    }
   ],
   "source": [
    "# from huggingface_hub import login\n",
    "# from huggingface_hub import interpreter_login\n",
    "\n",
    "# interpreter_login()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef83f6fc-2c21-46ee-a0b6-64c2205e3498",
   "metadata": {},
   "source": [
    "# Load your Model and Create Actual Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "amateur-lighting",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/fs01/home/ws_jinxiao/llm_finetuning/Deloitte/finetuned'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "6770bdb3-8d79-4f77-8cc1-97ffc5978e66",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/'"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.chdir(\"../../../../../../\")\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "e6f7eef3",
   "metadata": {},
   "outputs": [],
   "source": [
    "shared_space=\"/fs01/projects/fta_teams/deloitte\"\n",
    "base_model = \"/fs01/model-weights/Meta-Llama-3-8B-Instruct\"\n",
    "dataset_name = \"heliosbrahma/mental_health_conversational_dataset\"\n",
    "\n",
    "# new_model = \"/h/ws_aabboud/finetuning-and-alignment/Deloitte/finetuned/models/llama-3-8b-chat-doctor\"\n",
    "merged_model= f\"{shared_space}/merged_models/llama-3-8b-chat-doctor-merged-shyana-v1\"\n",
    "adapter_model=f\"{shared_space}/adapters/llama-3-8b-finetune-shyana-fulldatav1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1a81df03-881f-46f6-906f-df011aaf1393",
   "metadata": {},
   "outputs": [],
   "source": [
    "# base_model = \"/fs01/model-weights/Mistral-7B-v0.1\" # pretend this to be the fine-tuned model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "213e6917-a7c2-40e3-ba63-802f8f3c4ad1",
   "metadata": {},
   "source": [
    "### Set the data type and attention implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "abbc6971-4419-4c9a-b09f-461e9cbc8ade",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch_dtype = torch.float16\n",
    "attn_implementation = \"eager\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1411881d-6871-4365-bac0-b26815dea6bb",
   "metadata": {},
   "source": [
    "## Loading the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a20b5958-aa87-48e5-927b-83b1b3f5e954",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a66dc8df2afd49e4af8d2d60c972687c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# QLoRA config\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch_dtype,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    ")\n",
    "\n",
    "# Load model\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    base_model,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\",\n",
    "    attn_implementation=attn_implementation\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd73e419-e227-4a6a-ac71-79188848cf5e",
   "metadata": {},
   "source": [
    "## Load Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "506c8198-0ac6-4101-9a24-065af63f4d56",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "# Load tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(base_model)\n",
    "model, tokenizer = setup_chat_format(model, tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6fa12b7-1660-4ce3-b618-90e20aa64d61",
   "metadata": {},
   "source": [
    "## Adding the adapter to the layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4c9d7192-0ec2-4d9a-ae50-8a6f098d3f33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # LoRA config\n",
    "# peft_config = LoraConfig(\n",
    "#     r=16,\n",
    "#     lora_alpha=32,\n",
    "#     lora_dropout=0.05,\n",
    "#     bias=\"none\",\n",
    "#     task_type=\"CAUSAL_LM\",\n",
    "#     target_modules=['up_proj', 'down_proj', 'gate_proj', 'k_proj', 'q_proj', 'v_proj', 'o_proj']\n",
    "# )\n",
    "# model = get_peft_model(model, peft_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b432f1ee-cde2-4815-92ce-b2c26b1a401a",
   "metadata": {},
   "source": [
    "## Loading Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "aa617763-d5cb-43aa-af79-d6dc694e3ca5",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_name = \"heliosbrahma/mental_health_conversational_dataset\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "fbabc3e3-bfcc-487f-9a38-fbb0ef89bd13",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This function is to convert the dataset input to a LLLM acceptable input, \n",
    "then this acceptable input will be further converted into chat format by huggingface's apply_chat_template\n",
    "\"\"\"\n",
    "\n",
    "def extract_message(text, tag):\n",
    "    \"\"\"\n",
    "    Extracts a message from the given text based on the specified tag.\n",
    "    \n",
    "    Parameters:\n",
    "    text (str): The text containing the messages.\n",
    "    tag (str): The tag to search for, either '<<<HUMAN>>>: ' or '<<<ASSISTANT>>>: '.\n",
    "    \n",
    "    Returns:\n",
    "    str: The extracted message, or None if the tag is not found.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        start_tag = f'<<<{tag}>>>: '\n",
    "        end_tag = ' <<<'\n",
    "        \n",
    "        start_index = text.find(start_tag)\n",
    "        if start_index == -1:\n",
    "            return None\n",
    "        \n",
    "        start_index += len(start_tag)\n",
    "        end_index = text.find(end_tag, start_index)\n",
    "        \n",
    "        if end_index == -1:\n",
    "            # If end_tag not found, take the rest of the text\n",
    "            end_index = len(text)\n",
    "        \n",
    "        return text[start_index:end_index].strip()\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "abcbbb70-e7be-49f2-b600-ab0ee6cd7ff3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using the latest cached version of the dataset since heliosbrahma/mental_health_conversational_dataset couldn't be found on the Hugging Face Hub\n",
      "Found the latest cached dataset configuration 'default' at /h/ws_aabboud/.cache/huggingface/datasets/heliosbrahma___mental_health_conversational_dataset/default/0.0.0/fc8f9432d09ed6e54a3fc799daf7ff0a69a5d293 (last modified on Tue Jul 23 13:56:54 2024).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'<|im_start|>user\\nWhat are the side effects of medication?<|im_end|>\\n<|im_start|>assistant\\nLike other medication, psychiatric medication has its own set of side effects like Drowsiness, Restlessness, Dizziness, Dry mouth, Constipation, Nausea, and Vomiting.\\nIt is usually because of the body getting used to medication. It normally takes a month for the body to get used to these drugs. If you are feeling any of the above symptoms, call your doctor immediately.<|im_end|>\\n'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Importing the dataset\n",
    "dataset = load_dataset(dataset_name, split=\"all\")\n",
    "dataset = dataset.shuffle(seed=65).select(range(100)) # Only use 100 samples for quick demo\n",
    "\n",
    "def format_chat_template(row):\n",
    "    row_json = [{\"role\": \"user\", \"content\": extract_message(row[\"text\"], 'HUMAN')},\n",
    "               {\"role\": \"assistant\", \"content\":  extract_message(row[\"text\"], 'ASSISTANT')}]\n",
    "    row[\"text\"] = tokenizer.apply_chat_template(row_json, tokenize=False)\n",
    "    return row\n",
    "\n",
    "dataset = dataset.map(\n",
    "    format_chat_template,\n",
    "    num_proc=4,\n",
    ")\n",
    "\n",
    "dataset['text'][3]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acc8b899-09ce-4277-964d-0b6b7c95242a",
   "metadata": {},
   "source": [
    "### Split the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "fa2a2593-d478-4045-8c0e-34cd2585a163",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset.train_test_split(test_size=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "42b5d54a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['text'],\n",
       "        num_rows: 90\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['text'],\n",
       "        num_rows: 10\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "540ed274",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = dataset['train']\n",
    "val_dataset = dataset['test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "14b2b3a0-84d8-4289-b601-868500e5a1ed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/fs01/home/ws_aabboud/finetuning-and-alignment'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27762bd1-e16a-4b3c-bfb7-dfd5b5bc8603",
   "metadata": {},
   "source": [
    "## Produce Model Actual Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16e6be82-fc2a-468c-a9e0-fba5a9fbd880",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "cf98d7a0-21ab-4428-a1d8-d55585930e2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to extract a prompt like test input\n",
    "\n",
    "def extract_test_set_input_for_models(text):\n",
    "    # Define the regex pattern to match the assistant's output\n",
    "    pattern = r\"(<\\|im_start\\|>user\\n.*assistant\\n)\" \n",
    "    \n",
    "    # Use re.search to find the match\n",
    "    match = re.search(pattern, text, re.DOTALL)\n",
    "    \n",
    "    if match:\n",
    "        # Extract the matched text\n",
    "        return match.group(1).strip()\n",
    "    else:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "e1e7eaad-19f1-4c8a-8397-22ed0fc39131",
   "metadata": {},
   "outputs": [],
   "source": [
    "extracted_test_input_question = extract_test_set_input_for_models(dataset[\"test\"][\"text\"][0])\n",
    "\n",
    "# adding the \\n breaker in the end, because the extraction is not able to extract the \\n\n",
    "extracted_test_input_question = f\"{extracted_test_input_question}\\n\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "7f0f4109-8d79-4d53-b24a-958b0752fe75",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<|im_start|>user\\nHow can I find help for an alcohol or drug use problem?<|im_end|>\\n<|im_start|>assistant\\n'"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "extracted_test_input_question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "616a5f39-b402-4f9b-8176-fdf361b13044",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|im_start|>user\n",
      "How can I find help for an alcohol or drug use problem?<|im_end|>\n",
      "<|im_start|>assistant\n",
      "                                                                                                                                                                                                                                                                \n"
     ]
    }
   ],
   "source": [
    "# Usually we need \n",
    "# messages = [{\"role\": \"user\", \"content\": \"Hello doctor, I have bad insomnia. How do I get rid of it?\"}]\n",
    "# prompt = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "\n",
    "\n",
    "pipe = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"auto\",\n",
    ")\n",
    "\n",
    "# I made a function extract_test_set_input_for_models to let you use it directly to the testset to produce what is similar to prompt\n",
    "outputs = pipe(extracted_test_input_question, max_new_tokens=256, do_sample=True, temperature=0.3, top_k=50, top_p=0.95) # you could change the max_new_token accordingly \n",
    "print(outputs[0][\"generated_text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "d9cc4767-e2cd-4268-a00d-3e7056a2bcee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'generated_text': '<|im_start|>user\\nHow can I find help for an alcohol or drug use problem?<|im_end|>\\n<|im_start|>assistant\\n                                                                                                                                                                                                                                                                '}]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "b3c1a965-c91a-45ef-bd4c-add061ae69bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def extract_assistant_output(text):\n",
    "    # Define the regex pattern to match the assistant's output\n",
    "    pattern = r\"<\\|im_start\\|>assistant(.*)\"\n",
    "    \n",
    "    # Use re.search to find the match\n",
    "    match = re.search(pattern, text, re.DOTALL)\n",
    "    \n",
    "    if match:\n",
    "        # Extract the matched text\n",
    "        return match.group(1).strip()\n",
    "    else:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "b4355ec2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def extract_user_query(text):\n",
    "    # Define the regex pattern to capture the text between the 'user\\n' and '\\nassistant\\n'\n",
    "    pattern = r'(<\\|im_start\\|>user\\n.*?<\\|im_end\\|>)'\n",
    "    \n",
    "    # Search for the pattern in the text\n",
    "    match = re.search(pattern, text, re.DOTALL)\n",
    "    \n",
    "    # If a match is found, return the captured group (the user's query)\n",
    "    if match:\n",
    "        return match.group(1).strip()\n",
    "    else:\n",
    "        return None\n",
    "goldens = []\n",
    "for sentence in dataset[\"test\"][\"text\"]:\n",
    "    cleansed_input = extract_user_query(sentence)\n",
    "    golden_input = Golden(input=cleansed_input, context=[\"\"])\n",
    "    goldens.append(golden_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "d3b40056-7cb7-4950-bfb5-bf0a9e39895b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Use the function to extract the assistant's text\n",
    "extracted_text = extract_assistant_output(outputs[0][\"generated_text\"])\n",
    "print(extracted_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "722467a5-a5e1-4abe-b237-933e17dba477",
   "metadata": {},
   "source": [
    "# 2. Using DeepEval During Fine-tuning (Running into errors) regardless of using Huggingface model or Gemini"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1a86860-106c-4216-a90c-f31136481d0a",
   "metadata": {},
   "source": [
    "### create a DeepEval HuggingFace Callback (FAILED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "aaa99d4a-3383-463b-973c-5e7fa4b372f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModel\n",
    "\n",
    "\n",
    "\n",
    "# Jingy Code\n",
    "# eval_model_id =\"TheBloke/Mistral-7B-v0.1-GGUF\"\n",
    "# eval_file_name = \"mistral-7b-v0.1.Q5_K_M.gguf\"\n",
    "# eval_tokenizer = AutoTokenizer.from_pretrained(eval_model_id, gguf_file=eval_file_name)\n",
    "# eval_model = AutoModelForCausalLM.from_pretrained(eval_model_id, gguf_file=eval_file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "1c46c1fb-bbf8-4619-827f-521fd672da46",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a8e6a5980c014102a5985db539da8778",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Some parameters are on the meta device device because they were offloaded to the cpu.\n"
     ]
    }
   ],
   "source": [
    "# gguf_model= f\"{shared_space}/gguf_models/llama-3-8b-chat-doctor-merged-shyana-q8-v1.gguf\"\n",
    "# eval_tokenizer = AutoTokenizer.from_pretrained(\"llama-3-8b-chat-doctor-merged-shyana-q8-v1\", gguf_file=gguf_model)\n",
    "# eval_model = AutoModelForCausalLM.from_pretrained(\"llama-3-8b-chat-doctor-merged-shyana-q8-v1\", gguf_file=gguf_model)\n",
    "eval_tokenizer = AutoTokenizer.from_pretrained(merged_model)\n",
    "eval_model = AutoModelForCausalLM.from_pretrained(merged_model, device_map=\"auto\")\n",
    "terminators = [\n",
    "    tokenizer.eos_token_id,\n",
    "    tokenizer.convert_tokens_to_ids(\"<|eot_id|>\")\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98755d04-726b-49ae-9d48-a32a592f18e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from deepeval.models.base_model import DeepEvalBaseLLM\n",
    "\n",
    "class Mistral7B(DeepEvalBaseLLM):\n",
    "    def __init__(\n",
    "        self,\n",
    "        model,\n",
    "        tokenizer\n",
    "    ):\n",
    "        self.model = model\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "    def load_model(self):\n",
    "        return self.model\n",
    "\n",
    "    def generate(self, prompt: str) -> str:\n",
    "        model = self.load_model()\n",
    "\n",
    "        device = \"cuda\" # the device to load the model onto\n",
    "\n",
    "        model_inputs = self.tokenizer([prompt], return_tensors=\"pt\").to(device)\n",
    "        model.to(device)\n",
    "\n",
    "        generated_ids = model.generate(**model_inputs, \n",
    "                                       max_length=1000, \n",
    "                                       # max_new_tokens=1000, \n",
    "                                       do_sample=True)\n",
    "        return self.tokenizer.batch_decode(generated_ids)[0]\n",
    "\n",
    "    async def a_generate(self, prompt: str) -> str:\n",
    "        return self.generate(prompt)\n",
    "\n",
    "    def get_model_name(self):\n",
    "        return \"Mistral 7B\"\n",
    "\n",
    "mistral_7b = Mistral7B(model=eval_model, tokenizer=eval_tokenizer)\n",
    "print(mistral_7b.generate(\"Write me a joke\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "ef2df5f0-fca7-45b3-8e9d-58bdc2331d52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use evalatuion metric\n",
    "# answer_relevancy_metric = AnswerRelevancyMetric(model=mistral_7b)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "cffcc707",
   "metadata": {},
   "outputs": [],
   "source": [
    "## OpenAI\n",
    "os.environ[\"OPENAI_API_KEY\"] =os.getenv('OPENAI_API_KEY')\n",
    "# use evalatuion metric\n",
    "answer_relevancy_metric = AnswerRelevancyMetric()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "94d2c0ad-95b1-46c4-b9ef-e8b5bdffea75",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "EvaluationDataset(test_cases=[], goldens=[Golden(input='<|im_start|>user\\nHow can I find help for an alcohol or drug use problem?<|im_end|>', actual_output=None, expected_output=None, context=[''], retrieval_context=None, additional_metadata=None, comments=None, source_file=None), Golden(input='<|im_start|>user\\nHow common are mental illnesses?<|im_end|>', actual_output=None, expected_output=None, context=[''], retrieval_context=None, additional_metadata=None, comments=None, source_file=None), Golden(input='<|im_start|>user\\nAre there any ethnic/racial groups that more likely to have mental illnesses?<|im_end|>', actual_output=None, expected_output=None, context=[''], retrieval_context=None, additional_metadata=None, comments=None, source_file=None), Golden(input='<|im_start|>user\\nHow much alcohol is considered “too much”?<|im_end|>', actual_output=None, expected_output=None, context=[''], retrieval_context=None, additional_metadata=None, comments=None, source_file=None), Golden(input='<|im_start|>user\\nWhen a Child Needs Mental Health Assessment?<|im_end|>', actual_output=None, expected_output=None, context=[''], retrieval_context=None, additional_metadata=None, comments=None, source_file=None), Golden(input='<|im_start|>user\\nHow do I know if I’m unwell?<|im_end|>', actual_output=None, expected_output=None, context=[''], retrieval_context=None, additional_metadata=None, comments=None, source_file=None), Golden(input='<|im_start|>user\\nIs self-management right for me?<|im_end|>', actual_output=None, expected_output=None, context=[''], retrieval_context=None, additional_metadata=None, comments=None, source_file=None), Golden(input='<|im_start|>user\\nWhat is self-management?<|im_end|>', actual_output=None, expected_output=None, context=[''], retrieval_context=None, additional_metadata=None, comments=None, source_file=None), Golden(input='<|im_start|>user\\nWhat’s the difference between psychotherapy and counselling?<|im_end|>', actual_output=None, expected_output=None, context=[''], retrieval_context=None, additional_metadata=None, comments=None, source_file=None), Golden(input='<|im_start|>user\\nHow to Use Yoga to Improve Your Mental Health?<|im_end|>', actual_output=None, expected_output=None, context=[''], retrieval_context=None, additional_metadata=None, comments=None, source_file=None)], conversational_goldens=[], _alias=None, _id=None)"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_dataset = EvaluationDataset(goldens=goldens)\n",
    "eval_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "153fb7b2-9c23-4acb-b420-ae1645ef92cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from deepeval.metrics import GEval\n",
    "from deepeval.test_case import LLMTestCaseParams\n",
    "\n",
    "correctness_metric = GEval(\n",
    "    name=\"Correctness\",\n",
    "    criteria=\"Determine whether the actual output is relevant based on the expected output.\",\n",
    "    # NOTE: you can only provide either criteria or evaluation_steps, and not both\n",
    "    evaluation_steps=[\n",
    "        \"Check whether the facts in 'actual output' contradicts any facts in 'expected output'\",\n",
    "        \"You should also heavily penalize omission of detail\",\n",
    "        \"Vague language, or contradicting OPINIONS, are OK\"\n",
    "    ],\n",
    "    evaluation_params=[LLMTestCaseParams.INPUT, LLMTestCaseParams.ACTUAL_OUTPUT],\n",
    "    # model=mistral_7b\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "2ac71169-d0c6-4fe5-a872-6c9a409fd37a",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer_args = {\n",
    "    'max_length': 512,  # Adjust as needed\n",
    "    'return_tensors': 'pt',\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "473abfe2-ca86-4cd1-88f4-ba23284d3afa",
   "metadata": {},
   "outputs": [],
   "source": [
    "deepeval_hugging_face_callback = DeepEvalHuggingFaceCallback(\n",
    "    evaluation_dataset=eval_dataset,\n",
    "    # metrics=[correctness_metric],\n",
    "    metrics=[answer_relevancy_metric],\n",
    "    trainer=trainer,\n",
    "    tokenizer_args = tokenizer_args\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "f94b681f-ea28-49c3-b4da-545f82682dac",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Add DeepEval Callback\n",
    "trainer.add_callback(deepeval_hugging_face_callback)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6620e6f-07ff-405a-b4c7-4929daa16961",
   "metadata": {},
   "source": [
    "# 3. Evaluate the model after fine-tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2025694c-696a-4b8c-9484-09b90fa22fe9",
   "metadata": {},
   "source": [
    "## Using Deepeval with Gemini (Very Slow if no OpenAI key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4d04b1ea-0a16-4b3d-9f8f-a116dfe63dd5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "eb1ad3b2-a451-4686-a40e-f94f9b8e04d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load_ext autoreload\n",
    "# %autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6eafab7d-b7fe-4a93-bde4-1bf7dc9a6bc6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/fs01/home/ws_jinxiao/llm_finetuning/Deloitte/finetuned'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "home=os.path.expanduser('~')\n",
    "os.chdir(f'{home}/llm_finetuning/Deloitte/finetuned/')\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "c3bf27e1-a168-4aaf-9157-57989a7141f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Retrying langchain_google_genai.chat_models._chat_with_retry.<locals>._chat_with_retry in 2.0 seconds as it raised InternalServerError: 500 An internal error has occurred. Please retry or report in https://developers.generativeai.google/guide/troubleshooting.\n",
      "Retrying langchain_google_genai.chat_models._chat_with_retry.<locals>._chat_with_retry in 4.0 seconds as it raised InternalServerError: 500 An internal error has occurred. Please retry or report in https://developers.generativeai.google/guide/troubleshooting.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Why don't scientists trust atoms? \n",
      "\n",
      "Because they make up everything! \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# try Gemini  -- Skip this cell\n",
    "\n",
    "# Setup Gemini Model\n",
    "# from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "# generation_config = {\n",
    "#     \"temperature\": 1,\n",
    "#     \"top_p\": 0.95,\n",
    "#     \"top_k\": 64,\n",
    "#     \"max_output_tokens\": 1024,\n",
    "#     \"max_new_tokens\":1024,\n",
    "#     \"response_mime_type\": \"text/plain\",\n",
    "#     }\n",
    "\n",
    "\n",
    "\n",
    "# gemini_model=ChatGoogleGenerativeAI(model=\"gemini-1.5-pro\", temperature=0.7, google_api_key=os.getenv('GEMINI_API_KEY'))\n",
    "\n",
    "# # initiatialize the  wrapper class\n",
    "# vertexai_gemini = GoogleVertexAI(model=gemini_model)\n",
    "# print(vertexai_gemini.generate(\"Write me a joke\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ac0baaa-c4bf-4eb0-8949-688b4283e87f",
   "metadata": {},
   "source": [
    "## Using OpenAI"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a695743e-97f0-4b8d-92ae-359db71e833b",
   "metadata": {},
   "source": [
    "firstly create a .env under the same folder\n",
    "secondly, open terminal in jupyter notebook, run\n",
    "\n",
    "deepeval set-azure-openai --openai-endpoint=<endpoint> \\\n",
    "    --openai-api-key=<api_key> \\\n",
    "    --deployment-name=<deployment_name> \\\n",
    "    --openai-api-version=<openai_api_version> \\\n",
    "    --model-version=<model_version>\n",
    "\n",
    "\n",
    "we will need to use openai key\n",
    "\n",
    "if you don't want to run deepeval command, you can also put those strings into .env file and use os.getenv() to load the enviornment variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff2e6dfc-e086-461e-9624-d1f1ed0b4254",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from langchain_openai import AzureChatOpenAI\n",
    "from deepeval.models.base_model import DeepEvalBaseLLM\n",
    "\n",
    "# class AzureOpenAI(DeepEvalBaseLLM):\n",
    "#     def __init__(\n",
    "#         self,\n",
    "#         model\n",
    "#     ):\n",
    "#         self.model = model\n",
    "\n",
    "#     def load_model(self):\n",
    "#         return self.model\n",
    "\n",
    "#     def generate(self, prompt: str) -> str:\n",
    "#         chat_model = self.load_model()\n",
    "#         return chat_model.invoke(prompt).content\n",
    "\n",
    "#     async def a_generate(self, prompt: str) -> str:\n",
    "#         chat_model = self.load_model()\n",
    "#         res = await chat_model.ainvoke(prompt)\n",
    "#         return res.content\n",
    "\n",
    "#     def get_model_name(self):\n",
    "#         return \"Custom Azure OpenAI Model\"\n",
    "\n",
    "# # Replace these with real values\n",
    "# custom_model = AzureChatOpenAI(\n",
    "#     openai_api_version=openai_api_version,\n",
    "#     azure_deployment=azure_deployment,\n",
    "#     azure_endpoint=azure_endpoint,\n",
    "#     openai_api_key=openai_api_key,\n",
    "# )\n",
    "# azure_openai = AzureOpenAI(model=custom_model)\n",
    "# print(azure_openai.generate(\"Write me a joke\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f20d7256-92c1-436a-ab16-74e7928116d6",
   "metadata": {},
   "source": [
    "### AnswerRelevancy Metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "761f9989-bef7-4363-a8d8-47961128eda3",
   "metadata": {},
   "outputs": [],
   "source": [
    "answer_relevancy_metric = AnswerRelevancyMetric(\n",
    "    threshold=0.7,\n",
    "    model=\"gpt-3.5-turbo\", # change to openai if you are using openai\n",
    "    include_reason=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "9615da66-71d9-41b4-a201-fa5df0cb3326",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cleanse User input to include only the question\n",
    "\n",
    "def extract_user_query(text):\n",
    "    # Define the regex pattern to capture the text between the 'user\\n' and '\\nassistant\\n'\n",
    "    pattern = r'<\\|im_start\\|>user(\\n.*?)<\\|im_end\\|>'\n",
    "    \n",
    "    # Search for the pattern in the text\n",
    "    match = re.search(pattern, text, re.DOTALL)\n",
    "    \n",
    "    # If a match is found, return the captured group (the user's query)\n",
    "    if match:\n",
    "        return match.group(1).strip()\n",
    "    else:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "a98ff005-baf9-4066-953a-72df424bb580",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "How does mental health affect physical health?\n"
     ]
    }
   ],
   "source": [
    "# apply the function and append a list of cleansed input\n",
    "\n",
    "cleansed_test_input = []\n",
    "for sentence in dataset[\"test\"][\"text\"]:\n",
    "    cleansed_test_input.append(extract_user_query(sentence))\n",
    "\n",
    "print(cleansed_test_input[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "863cda9c-9101-408f-8853-f4971921e4c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to cleanse the output\n",
    "\n",
    "def extract_assistant_answer(text):\n",
    "    # Define the regex pattern to capture the text between the 'user\\n' and '\\nassistant\\n'\n",
    "    pattern = r'<\\|im_start\\|>assistant(\\n.*?)<\\|im_end\\|>'\n",
    "    \n",
    "    # Search for the pattern in the text\n",
    "    match = re.search(pattern, text, re.DOTALL)\n",
    "    \n",
    "    # If a match is found, return the captured group (the user's query)\n",
    "    if match:\n",
    "        return match.group(1).strip()\n",
    "    else:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "24b2064f-4d6c-4cd5-a1c9-052a5e452d2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mental health and physical health go hand-in-hand. For example, physical exercise can improve your mood and reduce your anxiety, among many other benefits. Similarly, neglecting your mental health can lead to serious health complications, including heart disease, high blood pressure, a weakened immune system, obesity, and more. Depression can cause problems like insomnia and chronic fatigue.\n"
     ]
    }
   ],
   "source": [
    "# apply the assistant output\n",
    "\n",
    "cleansed_assistant_output = []\n",
    "for sentence in dataset[\"test\"][\"text\"]:\n",
    "    cleansed_assistant_output.append(extract_assistant_answer(sentence))\n",
    "\n",
    "print(cleansed_assistant_output[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "bf24981a-db17-4b51-baee-2d150c5f739b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_test_case_for_test_set(test_inputs, actual_outputs):\n",
    "    test_cases = []\n",
    "    for test_input, actual_output in zip(test_inputs, actual_outputs):\n",
    "        test_case = LLMTestCase(\n",
    "            input=test_input,\n",
    "            actual_output=actual_output,)\n",
    "        test_cases.append(test_case)\n",
    "    return test_cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "c6b15942-cafc-4d4a-9e5e-b6285d2755ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_cases = generate_test_case_for_test_set(cleansed_test_input, cleansed_assistant_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "d17de136-50fc-4f24-81dd-3be8e9e9f34e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LLMTestCase(input='How does mental health affect physical health?', actual_output='Mental health and physical health go hand-in-hand. For example, physical exercise can improve your mood and reduce your anxiety, among many other benefits. Similarly, neglecting your mental health can lead to serious health complications, including heart disease, high blood pressure, a weakened immune system, obesity, and more. Depression can cause problems like insomnia and chronic fatigue.', expected_output=None, context=None, retrieval_context=None, additional_metadata=None, comments=None)"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_cases[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "556bfdc3-80ba-477a-bc38-854716cc6e23",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Mental health and physical health go hand-in-hand. For example, physical exercise can improve your mood and reduce your anxiety, among many other benefits. Similarly, neglecting your mental health can lead to serious health complications, including heart disease, high blood pressure, a weakened immune system, obesity, and more. Depression can cause problems like insomnia and chronic fatigue.'"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_cases[0].actual_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e069774b-82d0-4f5b-8406-fbc41c1a219a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# due to the quota limit, we will use for loop to evaluate the test case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8c96f42-e6fe-4e5d-821f-e1a861fbd579",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Event loop is already running. Applying nest_asyncio patch to allow async execution...\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Event loop is already running. Applying nest_asyncio patch to allow async execution...\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Retrying langchain_google_genai.chat_models._achat_with_retry.&lt;locals&gt;._achat_with_retry in 2.0 seconds as it \n",
       "raised InternalServerError: 500 An internal error has occurred. Please retry or report in \n",
       "https://developers.generativeai.google/guide/troubleshooting.\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Retrying langchain_google_genai.chat_models._achat_with_retry.<locals>._achat_with_retry in 2.0 seconds as it \n",
       "raised InternalServerError: 500 An internal error has occurred. Please retry or report in \n",
       "https://developers.generativeai.google/guide/troubleshooting.\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Retrying langchain_google_genai.chat_models._achat_with_retry.&lt;locals&gt;._achat_with_retry in 4.0 seconds as it \n",
       "raised InternalServerError: 500 An internal error has occurred. Please retry or report in \n",
       "https://developers.generativeai.google/guide/troubleshooting.\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Retrying langchain_google_genai.chat_models._achat_with_retry.<locals>._achat_with_retry in 4.0 seconds as it \n",
       "raised InternalServerError: 500 An internal error has occurred. Please retry or report in \n",
       "https://developers.generativeai.google/guide/troubleshooting.\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Retrying langchain_google_genai.chat_models._achat_with_retry.&lt;locals&gt;._achat_with_retry in 8.0 seconds as it \n",
       "raised ResourceExhausted: 429 Resource has been exhausted (e.g. check quota)..\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Retrying langchain_google_genai.chat_models._achat_with_retry.<locals>._achat_with_retry in 8.0 seconds as it \n",
       "raised ResourceExhausted: 429 Resource has been exhausted (e.g. check quota)..\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Retrying langchain_google_genai.chat_models._achat_with_retry.&lt;locals&gt;._achat_with_retry in 16.0 seconds as it \n",
       "raised ResourceExhausted: 429 Resource has been exhausted (e.g. check quota)..\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Retrying langchain_google_genai.chat_models._achat_with_retry.<locals>._achat_with_retry in 16.0 seconds as it \n",
       "raised ResourceExhausted: 429 Resource has been exhausted (e.g. check quota)..\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Retrying langchain_google_genai.chat_models._achat_with_retry.&lt;locals&gt;._achat_with_retry in 2.0 seconds as it \n",
       "raised ResourceExhausted: 429 Resource has been exhausted (e.g. check quota)..\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Retrying langchain_google_genai.chat_models._achat_with_retry.<locals>._achat_with_retry in 2.0 seconds as it \n",
       "raised ResourceExhausted: 429 Resource has been exhausted (e.g. check quota)..\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Retrying langchain_google_genai.chat_models._achat_with_retry.&lt;locals&gt;._achat_with_retry in 4.0 seconds as it \n",
       "raised ResourceExhausted: 429 Resource has been exhausted (e.g. check quota)..\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Retrying langchain_google_genai.chat_models._achat_with_retry.<locals>._achat_with_retry in 4.0 seconds as it \n",
       "raised ResourceExhausted: 429 Resource has been exhausted (e.g. check quota)..\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Retrying langchain_google_genai.chat_models._achat_with_retry.&lt;locals&gt;._achat_with_retry in 8.0 seconds as it \n",
       "raised ResourceExhausted: 429 Resource has been exhausted (e.g. check quota)..\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Retrying langchain_google_genai.chat_models._achat_with_retry.<locals>._achat_with_retry in 8.0 seconds as it \n",
       "raised ResourceExhausted: 429 Resource has been exhausted (e.g. check quota)..\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Retrying langchain_google_genai.chat_models._achat_with_retry.&lt;locals&gt;._achat_with_retry in 16.0 seconds as it \n",
       "raised ResourceExhausted: 429 Resource has been exhausted (e.g. check quota)..\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Retrying langchain_google_genai.chat_models._achat_with_retry.<locals>._achat_with_retry in 16.0 seconds as it \n",
       "raised ResourceExhausted: 429 Resource has been exhausted (e.g. check quota)..\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Event loop is already running. Applying nest_asyncio patch to allow async execution...\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Event loop is already running. Applying nest_asyncio patch to allow async execution...\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Event loop is already running. Applying nest_asyncio patch to allow async execution...\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Event loop is already running. Applying nest_asyncio patch to allow async execution...\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Retrying langchain_google_genai.chat_models._achat_with_retry.&lt;locals&gt;._achat_with_retry in 2.0 seconds as it \n",
       "raised InternalServerError: 500 An internal error has occurred. Please retry or report in \n",
       "https://developers.generativeai.google/guide/troubleshooting.\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Retrying langchain_google_genai.chat_models._achat_with_retry.<locals>._achat_with_retry in 2.0 seconds as it \n",
       "raised InternalServerError: 500 An internal error has occurred. Please retry or report in \n",
       "https://developers.generativeai.google/guide/troubleshooting.\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Retrying langchain_google_genai.chat_models._achat_with_retry.&lt;locals&gt;._achat_with_retry in 2.0 seconds as it \n",
       "raised ResourceExhausted: 429 Resource has been exhausted (e.g. check quota)..\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Retrying langchain_google_genai.chat_models._achat_with_retry.<locals>._achat_with_retry in 2.0 seconds as it \n",
       "raised ResourceExhausted: 429 Resource has been exhausted (e.g. check quota)..\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Retrying langchain_google_genai.chat_models._achat_with_retry.&lt;locals&gt;._achat_with_retry in 4.0 seconds as it \n",
       "raised ResourceExhausted: 429 Resource has been exhausted (e.g. check quota)..\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Retrying langchain_google_genai.chat_models._achat_with_retry.<locals>._achat_with_retry in 4.0 seconds as it \n",
       "raised ResourceExhausted: 429 Resource has been exhausted (e.g. check quota)..\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Retrying langchain_google_genai.chat_models._achat_with_retry.&lt;locals&gt;._achat_with_retry in 8.0 seconds as it \n",
       "raised ResourceExhausted: 429 Resource has been exhausted (e.g. check quota)..\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Retrying langchain_google_genai.chat_models._achat_with_retry.<locals>._achat_with_retry in 8.0 seconds as it \n",
       "raised ResourceExhausted: 429 Resource has been exhausted (e.g. check quota)..\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Retrying langchain_google_genai.chat_models._achat_with_retry.&lt;locals&gt;._achat_with_retry in 16.0 seconds as it \n",
       "raised ResourceExhausted: 429 Resource has been exhausted (e.g. check quota)..\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Retrying langchain_google_genai.chat_models._achat_with_retry.<locals>._achat_with_retry in 16.0 seconds as it \n",
       "raised ResourceExhausted: 429 Resource has been exhausted (e.g. check quota)..\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Event loop is already running. Applying nest_asyncio patch to allow async execution...\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Event loop is already running. Applying nest_asyncio patch to allow async execution...\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Retrying langchain_google_genai.chat_models._achat_with_retry.&lt;locals&gt;._achat_with_retry in 2.0 seconds as it \n",
       "raised InternalServerError: 500 An internal error has occurred. Please retry or report in \n",
       "https://developers.generativeai.google/guide/troubleshooting.\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Retrying langchain_google_genai.chat_models._achat_with_retry.<locals>._achat_with_retry in 2.0 seconds as it \n",
       "raised InternalServerError: 500 An internal error has occurred. Please retry or report in \n",
       "https://developers.generativeai.google/guide/troubleshooting.\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Retrying langchain_google_genai.chat_models._achat_with_retry.&lt;locals&gt;._achat_with_retry in 2.0 seconds as it \n",
       "raised ResourceExhausted: 429 Resource has been exhausted (e.g. check quota)..\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Retrying langchain_google_genai.chat_models._achat_with_retry.<locals>._achat_with_retry in 2.0 seconds as it \n",
       "raised ResourceExhausted: 429 Resource has been exhausted (e.g. check quota)..\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Retrying langchain_google_genai.chat_models._achat_with_retry.&lt;locals&gt;._achat_with_retry in 4.0 seconds as it \n",
       "raised ResourceExhausted: 429 Resource has been exhausted (e.g. check quota)..\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Retrying langchain_google_genai.chat_models._achat_with_retry.<locals>._achat_with_retry in 4.0 seconds as it \n",
       "raised ResourceExhausted: 429 Resource has been exhausted (e.g. check quota)..\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Retrying langchain_google_genai.chat_models._achat_with_retry.&lt;locals&gt;._achat_with_retry in 8.0 seconds as it \n",
       "raised ResourceExhausted: 429 Resource has been exhausted (e.g. check quota)..\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Retrying langchain_google_genai.chat_models._achat_with_retry.<locals>._achat_with_retry in 8.0 seconds as it \n",
       "raised ResourceExhausted: 429 Resource has been exhausted (e.g. check quota)..\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Retrying langchain_google_genai.chat_models._achat_with_retry.&lt;locals&gt;._achat_with_retry in 16.0 seconds as it \n",
       "raised ResourceExhausted: 429 Resource has been exhausted (e.g. check quota)..\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Retrying langchain_google_genai.chat_models._achat_with_retry.<locals>._achat_with_retry in 16.0 seconds as it \n",
       "raised ResourceExhausted: 429 Resource has been exhausted (e.g. check quota)..\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Retrying langchain_google_genai.chat_models._achat_with_retry.&lt;locals&gt;._achat_with_retry in 32.0 seconds as it \n",
       "raised ResourceExhausted: 429 Resource has been exhausted (e.g. check quota)..\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Retrying langchain_google_genai.chat_models._achat_with_retry.<locals>._achat_with_retry in 32.0 seconds as it \n",
       "raised ResourceExhausted: 429 Resource has been exhausted (e.g. check quota)..\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f7af77d0d68e478db1150f6f84893526",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Event loop is already running. Applying nest_asyncio patch to allow async execution...\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Event loop is already running. Applying nest_asyncio patch to allow async execution...\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Retrying langchain_google_genai.chat_models._achat_with_retry.&lt;locals&gt;._achat_with_retry in 2.0 seconds as it \n",
       "raised InternalServerError: 500 An internal error has occurred. Please retry or report in \n",
       "https://developers.generativeai.google/guide/troubleshooting.\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Retrying langchain_google_genai.chat_models._achat_with_retry.<locals>._achat_with_retry in 2.0 seconds as it \n",
       "raised InternalServerError: 500 An internal error has occurred. Please retry or report in \n",
       "https://developers.generativeai.google/guide/troubleshooting.\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Retrying langchain_google_genai.chat_models._achat_with_retry.&lt;locals&gt;._achat_with_retry in 4.0 seconds as it \n",
       "raised InternalServerError: 500 An internal error has occurred. Please retry or report in \n",
       "https://developers.generativeai.google/guide/troubleshooting.\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Retrying langchain_google_genai.chat_models._achat_with_retry.<locals>._achat_with_retry in 4.0 seconds as it \n",
       "raised InternalServerError: 500 An internal error has occurred. Please retry or report in \n",
       "https://developers.generativeai.google/guide/troubleshooting.\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Retrying langchain_google_genai.chat_models._achat_with_retry.&lt;locals&gt;._achat_with_retry in 8.0 seconds as it \n",
       "raised InternalServerError: 500 An internal error has occurred. Please retry or report in \n",
       "https://developers.generativeai.google/guide/troubleshooting.\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Retrying langchain_google_genai.chat_models._achat_with_retry.<locals>._achat_with_retry in 8.0 seconds as it \n",
       "raised InternalServerError: 500 An internal error has occurred. Please retry or report in \n",
       "https://developers.generativeai.google/guide/troubleshooting.\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Retrying langchain_google_genai.chat_models._achat_with_retry.&lt;locals&gt;._achat_with_retry in 16.0 seconds as it \n",
       "raised InternalServerError: 500 An internal error has occurred. Please retry or report in \n",
       "https://developers.generativeai.google/guide/troubleshooting.\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Retrying langchain_google_genai.chat_models._achat_with_retry.<locals>._achat_with_retry in 16.0 seconds as it \n",
       "raised InternalServerError: 500 An internal error has occurred. Please retry or report in \n",
       "https://developers.generativeai.google/guide/troubleshooting.\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "metric_name = \"answer relevancy\"\n",
    "metric_name_column = [metric_name* len(test_cases)]\n",
    "test_case_input = []\n",
    "test_case_actual_output = []\n",
    "expected_output = []\n",
    "metric_scores = []\n",
    "metric_reasonings = []\n",
    "# columns = ['metric_name', 'model_input', 'model_actual_output', 'expected_output', 'metric_score', 'metric_reasoning']\n",
    "\n",
    "for test_case in test_cases:\n",
    "    test_case_input.append(test_case.input)\n",
    "    test_case_actual_output.append(test_case.actual_output)\n",
    "    expected_output.append(test_case.expected_output)\n",
    "    \n",
    "    answer_relevancy_metric.measure(test_case)\n",
    "    metric_scores.append(answer_relevancy_metric.score)\n",
    "    metric_reasonings.append(answer_relevancy_metric.reason)\n",
    "    time.sleep(60)\n",
    "\n",
    "# Create a dictionary with the lists\n",
    "data_dict = {\n",
    "    'metric_name': metric_name_column,\n",
    "    'model_input': test_case_input,\n",
    "    'test_case_actual_output': model_actual_output,\n",
    "    'expected_output': expected_output,\n",
    "    'metric_score': metric_scores,\n",
    "    'metric_reasoning': metric_reasonings,\n",
    "}\n",
    "\n",
    "# Create a DataFrame using the dictionary\n",
    "answer_relevancy_df = pd.DataFrame(data_dict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9611a5f-a737-46ac-81d3-9e72769e7815",
   "metadata": {},
   "outputs": [],
   "source": [
    "# answer_relevancy = create_test_case_evaluation_table(test_cases, answer_relevancy_metric, \"answer relevancy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51ad936b-e24b-4b11-969e-7127f0879318",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# evaluate test cases in bulk\n",
    "# eval_set = evaluate(test_cases[:1], [metric])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "ee94962b-8118-47c6-b011-5ab238df058b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TestResult(success=True, metrics_metadata=[MetricMetadata(metric='Answer Relevancy', threshold=0.7, success=True, score=1.0, reason='This is an excellent score!  Keep up the great work!', strict_mode=False, evaluation_model='Vertex AI Model', error=None, evaluation_cost=None)], input='How does mental health affect physical health?', actual_output='Mental health and physical health go hand-in-hand. For example, physical exercise can improve your mood and reduce your anxiety, among many other benefits. Similarly, neglecting your mental health can lead to serious health complications, including heart disease, high blood pressure, a weakened immune system, obesity, and more. Depression can cause problems like insomnia and chronic fatigue.', expected_output=None, context=None, retrieval_context=None)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# eval_set[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d34ed0ad-9bb7-4b7f-a5bb-4de3008498da",
   "metadata": {},
   "source": [
    "## Empathy Metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87896ef4-0e7f-46bc-97ae-a506eb7a5d0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "empathy_metric = GEval(\n",
    "    name=\"Empathy\",\n",
    "    criteria=\"Determine whether the actual output demonstrates empathy and emotional understanding based on the expected output.\",\n",
    "    evaluation_steps=[\n",
    "        \"Check whether the 'actual output' acknowledges and validates the emotions expressed in the 'expected output'\",\n",
    "        \"Assess whether the 'actual output' provides comfort, support, or understanding in response to the emotions expressed\",\n",
    "        \"Consider if the 'actual output' uses empathetic language and tone appropriate to the context\"\n",
    "    ],\n",
    "    evaluation_params=[LLMTestCaseParams.INPUT, LLMTestCaseParams.ACTUAL_OUTPUT],\n",
    "    model=\"gpt-3.5-turbo\", # change to openai if you are using openai\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df78cebc-01ab-4ff7-8ed8-86267a25f0c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test a single test case\n",
    "test_case = LLMTestCase(\n",
    "    input=\"The dog chased the cat up the tree, who ran up the tree?\",\n",
    "    actual_output=\"It depends, some might consider the cat, while others might argue the dog.\",\n",
    "    expected_output=\"The cat.\"\n",
    ")\n",
    "\n",
    "empathy_metric.measure(test_case)\n",
    "print(empathy_metric.score)\n",
    "print(empathy_metric.reason)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d01c0cdf-de32-453f-9b0e-3aec8e3fd771",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use the same test_cases generated by previous section\n",
    "metric_name = \"empathy\"\n",
    "metric_name_column = [metric_name* len(test_cases)]\n",
    "test_case_input = []\n",
    "test_case_actual_output = []\n",
    "expected_output = []\n",
    "metric_scores = []\n",
    "metric_reasonings = []\n",
    "# columns = ['metric_name', 'model_input', 'model_actual_output', 'expected_output', 'metric_score', 'metric_reasoning']\n",
    "\n",
    "for test_case in test_cases:\n",
    "    test_case_input.append(test_case.input)\n",
    "    test_case_actual_output.append(test_case.actual_output)\n",
    "    expected_output.append(test_case.expected_output)\n",
    "    \n",
    "    empathy_metric.measure(test_case)\n",
    "    metric_scores.append(empathy_metric.score)\n",
    "    metric_reasonings.append(empathy_metric.reason)\n",
    "    time.sleep(60)\n",
    "\n",
    "# Create a dictionary with the lists\n",
    "data_dict = {\n",
    "    'metric_name': metric_name_column,\n",
    "    'model_input': test_case_input,\n",
    "    'test_case_actual_output': test_case_actual_output,\n",
    "    'expected_output': expected_output,\n",
    "    'metric_score': metric_scores,\n",
    "    'metric_reasoning': metric_reasonings,\n",
    "}\n",
    "\n",
    "# Create a DataFrame using the dictionary\n",
    "empathy_df = pd.DataFrame(data_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ea4ebd6-48f8-4652-b547-ad3ac8648553",
   "metadata": {},
   "source": [
    "### Combine two metric into one dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14b82dc7-6fe1-4053-847e-e36b489d6fa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluation_df = pd.concat([answer_relevancy_df, empathy_df], ignore_index=True, sort=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7de5e06-001f-4668-ba9c-227e376be4f3",
   "metadata": {},
   "source": [
    "#### Answer Relevancy Score:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "305f6ece-b707-4679-8e2b-58ef12108faa",
   "metadata": {},
   "outputs": [],
   "source": [
    "average_score_for_answer_relevancy = answer_relevancy_df['metric_score'].mean()\n",
    "\n",
    "print(f\"The average score for answer relevancy metric is {average_score_for_answer_relevancy:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eeb29205-6c84-4572-a7cf-61abe473e241",
   "metadata": {},
   "source": [
    "#### Empathy Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3ee63cc-b5a4-4057-a3d0-24ee71619cb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "average_score_for_empathy = empathy_df['metric_score'].mean()\n",
    "\n",
    "print(f\"The average score for answer empathy metric is {average_score_for_empathy:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a70fe1b-8f02-4922-b2bc-6ffa74d54b51",
   "metadata": {},
   "source": [
    "## BackUp Solution - ROUGE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0496eee4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "118e4894-4947-458b-8a3f-156492062d4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9ebcc735-3148-4a49-aec9-7398432482f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "rouge = evaluate.load('rouge')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d0663288-222b-40bd-96b0-6288bb980800",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'dataset' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[18], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m dialogue \u001b[38;5;241m=\u001b[39m \u001b[43mdataset\u001b[49m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtest\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m      2\u001b[0m human_baseline_summaries \u001b[38;5;241m=\u001b[39m dataset[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtest\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msummary\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m      4\u001b[0m original_model_summaries \u001b[38;5;241m=\u001b[39m []\n",
      "\u001b[0;31mNameError\u001b[0m: name 'dataset' is not defined"
     ]
    }
   ],
   "source": [
    "dialogue = dataset['test']['text']\n",
    "human_baseline_summaries = dataset['test']['summary']\n",
    "\n",
    "original_model_summaries = []\n",
    "instruct_model_summaries = []\n",
    "\n",
    "for _, dialogue in enumerate(dialogue):\n",
    "    prompt = f\"\"\"\n",
    "    Summarize the following conversations. \n",
    "\n",
    "    {dialogue}\n",
    "\n",
    "    Summary: \"\"\"\n",
    "\n",
    "    input_ids = tokenizer(prompt, return_tensors='pt').input_ids\n",
    "\n",
    "    original_model_outputs = original_model.generate(input_ids=input_ids, generation_config=GenerationConfig(max_new_tokens=200, num_beams=1))\n",
    "    original_model_text_output = tokenizer.decode(original_model_outputs[0], skip_special_tokens=True)\n",
    "    original_model_summaries.append(original_model_text_output)\n",
    "    \n",
    "   \n",
    "\n",
    "zipped_summaries = list(zip(human_baseline_summaries, original_model_summaries))\n",
    "\n",
    "df = pd.DataFrame(zipped_summaries, columns=['human_baseline_summaries', 'original_model_summaries'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfa16601-520c-4c39-9f03-de9132a75767",
   "metadata": {},
   "outputs": [],
   "source": [
    "fintuned_model_results = rouge.compute(predictions=actual_outputs, \n",
    "                                       references=expected_outputs,\n",
    "                                      use_aggregator=True,\n",
    "\n",
    "print(f'Finetuned Model ROUGE scores: \\n{fintuned_model_results}\\n') "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
